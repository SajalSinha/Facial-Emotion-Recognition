{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FERWebApp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvlqUFw5kBz6OYcW0LL7sG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SajalSinha/Facial-Emotion-Recognition/blob/main/FERWebApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xttwauxA7D-x"
      },
      "source": [
        "#To run webapp using colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAtNnZF0DHPd"
      },
      "source": [
        "An attempt was made to see if we can deploy our webapp using google colab. Our experience and learning are mentioned in the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKqz0jx_De2A"
      },
      "source": [
        "# To link Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxKv9AooyNnA",
        "outputId": "599616f5-ef9f-4d55-8b1d-c17212108a3c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY6r2CiD7JLH"
      },
      "source": [
        "# Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWiwFA-syVNK"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmyMpPvkyXW9"
      },
      "source": [
        "!pip install streamlit_webrtc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks9w5zkDyeZX"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "044Vjc22yhpt",
        "outputId": "fe97c78e-5ad4-4732-debe-9cd8903d190b"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gySiOxxfzRLf"
      },
      "source": [
        "!pip install --upgrade ipykernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ak6OQqZzZft",
        "outputId": "92c2db6d-7aba-40c8-ce4f-ccd1b6be9f34"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4DyodAZ1CzX",
        "outputId": "4903e9fe-8aab-4082-b719-f10dcda6d4df"
      },
      "source": [
        "# Auth token to link with NgRok (Create your account and use the one alloted there)\n",
        "\n",
        "! ngrok authtoken 1xFnYtGBLJtnVaSpGLwo9qhCOZu_4gRapCrF7aHizdUiCkR1r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VFGJ5661Hev"
      },
      "source": [
        "!wget -N https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n",
        "!wget -N https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqNl3y787YlC"
      },
      "source": [
        "# To create .py file which will be our app"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFItPT3PykjC",
        "outputId": "3ab31622-9cf3-4173-f853-eb963d649ce4"
      },
      "source": [
        "%%writefile FER_WebApp.py\n",
        "# Importing required libraries, obviously\n",
        "import logging\n",
        "import logging.handlers\n",
        "import threading\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import model_from_json\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from streamlit_webrtc import VideoTransformerBase, webrtc_streamer\n",
        "import av\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    from typing import Literal\n",
        "except ImportError:\n",
        "    from typing_extensions import Literal  # type: ignore\n",
        "    \n",
        "def load_model(path):\n",
        "\n",
        "\tjson_file = open(path + 'model.json', 'r')\n",
        "\tloaded_model_json = json_file.read()\n",
        "\tjson_file.close()\n",
        "\t\n",
        "\tmodel = model_from_json(loaded_model_json)\n",
        "\tmodel.load_weights(path + \"model.h5\")\n",
        "\tprint(\"Loaded model from disk\")\n",
        "\treturn model\n",
        "\n",
        "# Loading pre-trained parameters for the cascade classifier\n",
        "try:\n",
        "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') # Face Detection\n",
        "    path = \"/content/drive/MyDrive/Capstone Project 5/\"\n",
        "    classifier = load_model(path)\n",
        "    # classifier =load_model('model.h5')  #Load model\n",
        "    emotion_labels = ['Angry','Disgust','Fear','Happy','Neutral', 'Sad', 'Surprise']  # Emotion that will be predicted\n",
        "except Exception:\n",
        "    st.write(\"Error loading cascade classifiers\")\n",
        "    \n",
        "    \n",
        "class VideoTransformer(VideoTransformerBase):\n",
        "    \n",
        "    \n",
        "\n",
        "    def transform(self, frame):\n",
        "        label=[]\n",
        "        img = frame.to_ndarray(format=\"bgr24\")\n",
        "        face_detect = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_frontalface_default.xml')\n",
        "        emotion_labels = ['Angry','Disgust','Fear','Happy','Neutral', 'Sad', 'Surprise']\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detect.detectMultiScale(gray, 1.3,1)\n",
        "        \n",
        "\n",
        "        for (x,y,w,h) in faces:\n",
        "            a=cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "            roi_gray = gray[y:y+h,x:x+w]\n",
        "            roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)  ##Face Cropping for prediction\n",
        "            roi = roi_gray.astype('float')/255.0\n",
        "            roi = img_to_array(roi)\n",
        "            roi = np.expand_dims(roi,axis=0) ## reshaping the cropped face image for prediction\n",
        "            prediction = classifier.predict(roi)[0]   #Prediction\n",
        "            label=emotion_labels[prediction.argmax()]\n",
        "            label_position = (x,y)\n",
        "            b=cv2.putText(a,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
        "               \n",
        "        return b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def face_detect():\n",
        "    class VideoTransformer(VideoTransformerBase):\n",
        "        frame_lock: threading.Lock  # `transform()` is running in another thread, then a lock object is used here for thread-safety.\n",
        "        in_image: Union[np.ndarray, None]\n",
        "        out_image: Union[np.ndarray, None]\n",
        "\n",
        "        def __init__(self) -> None:\n",
        "            self.frame_lock = threading.Lock()\n",
        "            self.in_image = None\n",
        "            self.out_image = None\n",
        "\n",
        "        def transform(self, frame: av.VideoFrame) -> np.ndarray:\n",
        "            in_image = frame.to_ndarray(format=\"bgr24\")\n",
        "\n",
        "            out_image = in_image[:, ::-1, :]  # Simple flipping for example.\n",
        "\n",
        "            with self.frame_lock:\n",
        "                self.in_image = in_image\n",
        "                self.out_image = out_image\n",
        "\n",
        "            return in_image\n",
        "\n",
        "    ctx = webrtc_streamer(key=\"snapshot\", video_processor_factory=VideoTransformer)\n",
        "\n",
        "    while ctx.video_transformer:\n",
        "        \n",
        "        \n",
        "            with ctx.video_transformer.frame_lock:\n",
        "                in_image = ctx.video_transformer.in_image\n",
        "                out_image = ctx.video_transformer.out_image\n",
        "\n",
        "            if in_image is not None :\n",
        "                gray = cv2.cvtColor(in_image, cv2.COLOR_BGR2GRAY)\n",
        "                faces = face_classifier.detectMultiScale(gray)\n",
        "                for (x,y,w,h) in faces:\n",
        "                    a=cv2.rectangle(in_image,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "                    roi_gray = gray[y:y+h,x:x+w]\n",
        "                    roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)  ##Face Cropping for prediction\n",
        "                    if np.sum([roi_gray])!=0:\n",
        "                        roi = roi_gray.astype('float')/255.0\n",
        "                        roi = img_to_array(roi)\n",
        "                        roi = np.expand_dims(roi,axis=0) ## reshaping the cropped face image for prediction\n",
        "                        prediction = classifier.predict(roi)[0]   #Prediction\n",
        "                        label=emotion_labels[prediction.argmax()]\n",
        "                        label_position = (x,y)\n",
        "                        b=cv2.putText(a,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)   # Text Adding\n",
        "                        st.image(b,channels=\"BGR\")\n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "from streamlit_webrtc import (\n",
        "    ClientSettings,\n",
        "    VideoTransformerBase,\n",
        "    WebRtcMode,\n",
        "    webrtc_streamer,\n",
        ")\n",
        "\n",
        "HERE = Path(__file__).parent\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "WEBRTC_CLIENT_SETTINGS = ClientSettings(\n",
        "    rtc_configuration={\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]},\n",
        "    media_stream_constraints={\"video\": True, \"audio\": True},\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "def about():\n",
        "\tst.write(\n",
        "\t\t'''\n",
        "\t\t**Haar Cascade** is an object detection algorithm.\n",
        "\t\tIt can be used to detect objects in images or videos.\n",
        "        \n",
        "\t\tThe algorithm has four stages:\n",
        "            \n",
        "\t\t\t1. Haar Feature Selection\n",
        "            \n",
        "\t\t\t2. Creating  Integral Images\n",
        "            \n",
        "\t\t\t3. Adaboost Training\n",
        "            \n",
        "\t\t\t4. Cascading Classifiers\n",
        "            \n",
        "Read more :\n",
        "    point_right: \n",
        "        https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html\n",
        "https://sites.google.com/site/5kk73gpu2012/assignment/viola-jones-face-detection#TOC-Image-Pyramid\n",
        "\t\t''')\n",
        "def app_video_filters():\n",
        "    \"\"\" Video transforms with OpenCV \"\"\"\n",
        "\n",
        "    class OpenCVVideoTransformer(VideoTransformerBase):\n",
        "        type: Literal[\"noop\", \"cartoon\", \"edges\", \"rotate\"]\n",
        "\n",
        "        def __init__(self) -> None:\n",
        "            self.type = \"noop\"\n",
        "\n",
        "        def transform(self, frame: av.VideoFrame) -> av.VideoFrame:\n",
        "            img = frame.to_ndarray(format=\"bgr24\")\n",
        "\n",
        "            if self.type == \"noop\":\n",
        "                pass\n",
        "            elif self.type == \"cartoon\":\n",
        "                # prepare color\n",
        "                img_color = cv2.pyrDown(cv2.pyrDown(img))\n",
        "                for _ in range(6):\n",
        "                    img_color = cv2.bilateralFilter(img_color, 9, 9, 7)\n",
        "                img_color = cv2.pyrUp(cv2.pyrUp(img_color))\n",
        "\n",
        "                # prepare edges\n",
        "                img_edges = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "                img_edges = cv2.adaptiveThreshold(\n",
        "                    cv2.medianBlur(img_edges, 7),\n",
        "                    255,\n",
        "                    cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "                    cv2.THRESH_BINARY,\n",
        "                    9,\n",
        "                    2,\n",
        "                )\n",
        "                img_edges = cv2.cvtColor(img_edges, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "                # combine color and edges\n",
        "                img = cv2.bitwise_and(img_color, img_edges)\n",
        "            elif self.type == \"edges\":\n",
        "                # perform edge detection\n",
        "                img = cv2.cvtColor(cv2.Canny(img, 100, 200), cv2.COLOR_GRAY2BGR)\n",
        "            elif self.type == \"rotate\":\n",
        "                # rotate image\n",
        "                rows, cols, _ = img.shape\n",
        "                M = cv2.getRotationMatrix2D((cols / 2, rows / 2), frame.time * 45, 1)\n",
        "                img = cv2.warpAffine(img, M, (cols, rows))\n",
        "\n",
        "            return img\n",
        "\n",
        "    webrtc_ctx = webrtc_streamer(\n",
        "        key=\"opencv-filter\",\n",
        "        mode=WebRtcMode.SENDRECV,\n",
        "        client_settings=WEBRTC_CLIENT_SETTINGS,\n",
        "        video_transformer_factory=OpenCVVideoTransformer,\n",
        "        async_transform=True,\n",
        "    )\n",
        "\n",
        "    if webrtc_ctx.video_transformer:\n",
        "        webrtc_ctx.video_transformer.type = st.radio(\n",
        "            \"Select transform type\", (\"noop\", \"cartoon\", \"edges\", \"rotate\")\n",
        "        )\n",
        "\n",
        "    st.markdown(\n",
        "        \"This demo is based on \"\n",
        "        \"https://github.com/aiortc/aiortc/blob/2362e6d1f0c730a0f8c387bbea76546775ad2fe8/examples/server/server.py#L34. \"  # noqa: E501\n",
        "        \"Many thanks to the project.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    activities = [\"Introduction\",\"Home\",\"Real-Time Snapshot\", \"Check Camera\",\"About\",\"Contact Us\"]\n",
        "    choice = st.sidebar.selectbox(\"Pick something Useful\", activities)\n",
        "    \n",
        "\n",
        "    if choice == \"Real-Time Snapshot\":\n",
        "        html_temp = \"\"\"\n",
        "    <body style=\"background-color:red;\">\n",
        "    <div style=\"background-color:teal ;padding:10px\">\n",
        "    <h2 style=\"color:white;text-align:center;\">Face Emotion Recognition WebApp</h2>\n",
        "    </div>\n",
        "    </body>\n",
        "        \"\"\"\n",
        "        st.markdown(html_temp, unsafe_allow_html=True)\n",
        "        st.write(\"**Using the Haar cascade Classifiers**\")\n",
        "        st.write(\"Go to the About section from the sidebar to learn more about it.\")\n",
        "        st.write(\"**Instructions while using the APP**\")\n",
        "        st.write('''\n",
        "                  \n",
        "                  1. Click on the Start button to start.\n",
        "                 \n",
        "                  2. WebCam window will open  automatically. \n",
        "\t\t  \n",
        "\t\t          3. It will automatically throw the image with the prediction at that instant.\n",
        "                  \n",
        "                  4. Make sure that camera shouldn't be used by any other app.\n",
        "                  \n",
        "                  5. For live recognition the app is getting slow and takes more time to predict and couldn't predict easily thus fluctuating the result.\n",
        "\t\t             Thus Taking a snapshot at any instant of time and it will automatically predict and give the picture with prediction.\n",
        "\t\t     \n",
        "\t\t          6. Easy to know what was or what is the emotion at a particular time.\n",
        "                  \n",
        "                  7. Click on  Stop  to end.\n",
        "                  \n",
        "                  8. Still webcam window didnot open,  go to Check Camera from the sidebar.''')\n",
        "        \n",
        "        \n",
        "        face_detect()\n",
        "        \n",
        "    elif choice ==\"Home\":\n",
        "        html_temp = \"\"\"\n",
        "    <body style=\"background-color:red;\">\n",
        "    <div style=\"background-color:teal ;padding:10px\">\n",
        "    <h2 style=\"color:white;text-align:center;\">Face Emotion Recognition WebApp</h2>\n",
        "    </div>\n",
        "    </body>\n",
        "        \"\"\"\n",
        "        st.markdown(html_temp, unsafe_allow_html=True)\n",
        "        st.write(\"**Using the Haar cascade Classifiers**\")\n",
        "        st.write(\"Go to the About section from the sidebar to learn more about it.\")\n",
        "        st.write(\"**Instructions while using the APP**\")\n",
        "        st.write(''' \n",
        "                  1. Click on the Start button to start.\n",
        "                 \n",
        "                  2. WebCam window will open  automatically. \n",
        "\t\t  \n",
        "\t\t          3. It will automatically  predict at that instant.\n",
        "                  \n",
        "                  4. Make sure that camera shouldn't be used by any other app.\n",
        "                  \n",
        "                  5. Click on  Stop  to end.\n",
        "                \n",
        "\t\t          6. Still webcam window didnot open,  go to Check Camera from the sidebar.\n",
        "              \n",
        "              ''')\n",
        "        webrtc_streamer(key=\"example\", video_transformer_factory=VideoTransformer)\n",
        "        \n",
        "        \n",
        "    \n",
        "    \n",
        "    elif choice == \"Check Camera\":\n",
        "        html_temp = \"\"\"\n",
        "    <body style=\"background-color:red;\">\n",
        "    <div style=\"background-color:teal ;padding:10px\">\n",
        "    <h2 style=\"color:white;text-align:center;\">Check Webcam is working or not</h2>\n",
        "    </div>\n",
        "    </body>\n",
        "        \"\"\"\n",
        "        st.markdown(html_temp, unsafe_allow_html=True)\n",
        "        st.write(\"**Instructions while Checking Camrea**\")\n",
        "        st.write('''\n",
        "  \n",
        "                  1. Click on  Start  to open webcam.\n",
        "                 \n",
        "                  2. If you have more than one camera , then select by using select device.\n",
        "\t\t  \n",
        "\t\t          3. Have some fun with your camera by choosing the options below.\n",
        "                  \n",
        "                  4. Click on  Stop  to end.\n",
        "                  \n",
        "                  5. Still webcam window didnot open,  Contact Us.''')\n",
        "        app_video_filters()\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    elif choice == \"About\":\n",
        "        \n",
        "        html_temp = \"\"\"\n",
        "    <body style=\"background-color:red;\">\n",
        "    <div style=\"background-color:teal ;padding:10px\">\n",
        "    <h2 style=\"color:white;text-align:center;\">Haar Cascade Object Detection</h2>\n",
        "    </div>\n",
        "    </body>\n",
        "        \"\"\"\n",
        "        st.markdown(html_temp, unsafe_allow_html=True)\n",
        "        about()\n",
        "    elif choice==\"Contact Us\":\n",
        "        with st.form(key='my_form'):\n",
        "            text_input = st.text_input(label='Enter some text')\n",
        "            submit_button = st.form_submit_button(label='Submit')\n",
        "        st.write('''\n",
        "                  Email:- 1) sajals1146@gmail.com.\n",
        "                          2) arifuddinatif@gmail.com\n",
        "                  Linkedin:-https://www.linkedin.com/in/sajal-sinha/\n",
        "                  \n",
        "                  ''')\n",
        "        \n",
        "        html_temp = \"\"\"\n",
        "    <body style=\"background-color:white;\">\n",
        "    <div style=\"background-color:red ;padding:0.25px\">\n",
        "    <h4 style=\"color:white;text-align:center;\">Copyright © 2021 | Sajal Atif </h4>\n",
        "    </div>\n",
        "    </body>\n",
        "        \"\"\"\n",
        "        st.markdown(html_temp, unsafe_allow_html=True)\n",
        "    elif choice==\"Introduction\":\n",
        "         html_temp = \"\"\"\n",
        "    <body style=\"background-color:white;\">\n",
        "    <h1 style=\"text-align:center;\"> Face Emotion Recognitiom </h1>\n",
        "    <h3 style=\"color:black;text-align:center;\">During online classes students often tends to loose attention, which leads to\n",
        "overall non-productivity. For a teacher, its often important for its students to easily\n",
        "grasp concept taught by them. Teachers have skills to observe their students and im-\n",
        "prove their way throughout their teaching. But due to online teaching, observing has\n",
        "become tough which has eventually disturbed student teacher balance and teaching\n",
        "methods. So, our aim was to develop a Face-Emotion-Recognition Model which can be\n",
        "used a micro service as well so that teachers can understand students much better and\n",
        "enlighten the way to teach.</h3>\n",
        "    <h3 style=\"color:red;text-align:center;\">To Know your emotion proceed to Home from the side bar.</h3>\n",
        "    </div>\n",
        "    </body>\n",
        "        \"\"\"\n",
        "         st.markdown(html_temp, unsafe_allow_html=True)\n",
        "        \n",
        "        \n",
        "  \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting FER_WebApp.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlp7XnHX7f1o"
      },
      "source": [
        "### Library to get requirement.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKBkalkqkfA0",
        "outputId": "4dd2ca3a-d5dc-45c9-a449-f3bb5b25ce60"
      },
      "source": [
        "!pip install pipreqs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.4.10-py2.py3-none-any.whl (25 kB)\n",
            "Collecting yarg\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from pipreqs) (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yarg->pipreqs) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2021.5.30)\n",
            "Installing collected packages: yarg, pipreqs\n",
            "Successfully installed pipreqs-0.4.10 yarg-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_xBejemkqaf",
        "outputId": "7eaca77f-1a3c-4fe9-ef1c-0b07ab5ffdad"
      },
      "source": [
        "!pipreqs ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Successfully saved requirements file in ./requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd9_zD0A00p_",
        "outputId": "b0b1524f-22e2-46db-8700-a4efd116a110"
      },
      "source": [
        "#This will allow our app to run in background\n",
        "\n",
        "!nohup streamlit run --server.port 80 FER_WebApp.py &"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EKb-0GO1TmM",
        "outputId": "02f0999a-942c-404a-830e-11483b6c963e"
      },
      "source": [
        "# To establish link/tunnel with local server and ngrok free server.\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "url = ngrok.connect(port=8501)\n",
        "url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://31bd-34-91-174-120.ngrok.io\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TByWMip11YFv"
      },
      "source": [
        "# Kills current webapp\n",
        "\n",
        "ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX2pntbl1k2f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}